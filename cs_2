from selenium import webdriver
from bs4 import BeautifulSoup
import datetime
import time
import random
import json
import pandas as pnds
import re
import hashlib
import csv
import requests
import os


def get_urls():
    start_date = '2022-10-16'
    end_date = '2022-10-25'
    daterange = pnds.date_range(start_date, end_date)
    s = pnds.to_datetime(start_date)
    e = pnds.to_datetime(end_date)
    start_time = time.time()
    urls = []
    count = 1
    driver = webdriver.Chrome(executable_path="C:\\Users\\Admin\\PycharmProjects\\betpebet\\chromedriver\\chromedriver.exe")
    for single_date in daterange:
        driver.get('https://www.cybersport.ru/api/matches?sort=-date&filter[disciplineIds]=21&filter[isFinished]=true&filter[date]=' + single_date.strftime("%Y-%m-%d"))
        time.sleep(random.randrange(2, 5))
        if count % 10 == 0:
            time.sleep(random.randrange(5, 9))
        soup = BeautifulSoup(driver.page_source, 'lxml')
        urls_json = json.loads(soup.text)
        for id in urls_json['data']:
            urls.append(id['id'])
        try:
            print(urls[-1])
        except:
            continue
        finally:
            print(f'[+] Processed: {round(count / (int(str(e - s).split(" ")[0]) + 1) * 100, 2)}')
            count += 1
    driver.close()
    driver.quit()
    with open('D:\\newData\\betpebet\\cybersport\\results\\urls_.json', 'w', encoding='utf-8') as file:
        json.dump(urls, file, indent=4, ensure_ascii=False)
    #for url in urls:
    #    with open('D:\\newData\\betpebet\\cybersport\\results\\2021\\urls_may.csv', 'a') as file:
    #        writer = csv.writer(file)
    #        writer.writerow(url)
    print(round((time.time() - start_time) / 60, 2))
    return '[INFO] Data collected successfully'


def get_data():
    year = '2022'
    start_time = time.time()
    result_list = []
    count = 1
    data = pnds.read_csv('D:\\newData\\betpebet\\cybersport\\results\\' + year + '\\' + year + '.csv')
    matches = list(data.urls_list)
    driver = webdriver.Chrome(executable_path="C:\\Users\\Admin\\PycharmProjects\\betpebet\\chromedriver\\chromedriver.exe")
    for matchid in matches:
        url = 'https://www.cybersport.ru/matches/dota-2/' + str(matchid)
        driver.get(url)
        time.sleep(random.randrange(2, 5))
        if count % 10 == 0:
            time.sleep(random.randrange(5, 9))
        os.mkdir('D:\\newData\\betpebet\\cybersport\\results\\' + year + '\\' + str(matchid))
        soup = BeautifulSoup(driver.page_source, 'lxml')
        if 'matchStatsDota' in str(soup):
            elements = driver.find_elements_by_xpath("//div[starts-with(@class, 'card_')]")
            scroll = driver.find_elements_by_xpath("//div[starts-with(@class, 'scroll-btn scroll-btn--right scroll-btn--squar')]")
            for element in elements:
                driver.execute_script("arguments[0].click();", element)
                soup = BeautifulSoup(driver.page_source, 'lxml')
                for s in soup.select('script'):
                    s.extract()
                my_file = open('D:\\newData\\betpebet\\cybersport\\results\\' + year + '\\' + str(matchid) + '\\f' + str(elements.index(element) + 1) + '.txt', 'w', encoding="utf-8")
                my_file.write(str(soup))
                my_file.close()
        else:
            for s in soup.select('script'):
                s.extract()
            my_file = open('D:\\newData\\betpebet\\cybersport\\results\\' + year + '\\' + str(matchid) + '\\f0.txt', 'w', encoding="utf-8")
            my_file.write(str(soup))
            my_file.close()
        print(f'[+] Processed: {round(count / len(matches) * 100, 2)}')
        count += 1

    driver.close()
    driver.quit()
    print(round((time.time() - start_time) / 60, 2))
    return '[INFO] Data collected successfully'


def dataCollect():
    folders = os.listdir('D:\\newData\\betpebet\\cybersport\\results\\2013')
    for folder in folders:
        with open('D:\\newData\\betpebet\\cybersport\\results\\2013\\' + folder + '\\f0.txt', 'r', encoding='utf8') as f:
            html = f.read()
            soup = BeautifulSoup(html, 'lxml')
            for s in soup.select('svg'):
                s.extract()
            for s in soup.select('path'):
                s.extract()
            match_header = soup.find('div', class_ = re.compile('^rounded-block matchInfo_'))
            t1 = match_header.find_all('div', class_ = re.compile('^participantTitle'))[0].text.strip()
            t2 = match_header.find_all('div', class_ = re.compile('^participantTitle'))[1].text.strip()
            ts = match_header.find('div', class_=re.compile('^matchScore')).get_text().strip()
            t1s = ts.split(':')[0]
            t2s = ts.split(':')[1]
            ed = match_header.find('div', class_=re.compile('^matchTime')).get_text().replace(' Ð² ', ' ').strip()
            t = match_header.find('small').get_text().strip()
            r = match_header.find('a', class_=re.compile('^matchSubtitle')).get_text().strip()
            bo = match_header.find('div', class_=re.compile('^matchFormat')).get_text().strip()
            print(bo)


def get_results():
    pass


def make_directory():
    os.mkdir('D:\\newData\\betpebet\\cybersport\\results\\2021\\' + '2316')


def dates():
    start_date = '2022-05-01'
    end_date = '2022-05-02'
    s = pnds.to_datetime(start_date)
    e = pnds.to_datetime(end_date)
    daterange = pnds.date_range(start_date, end_date)
    #for single_date in daterange:
    #    print(single_date.strftime("%Y-%m-%d"))
    print(str(e - s).split(' ')[0])


def get_data1():
    driver = webdriver.Chrome(executable_path="C:\\Users\\Admin\\PycharmProjects\\betpebet\\chromedriver\\chromedriver.exe")
    url = 'https://www.cybersport.ru/matches/dota-2/55401'
    driver.get(url)
    soup = BeautifulSoup(driver.page_source, 'lxml')
    for s in soup.select('script'):
        s.extract()
    print(soup)

    driver.close()
    driver.quit()


def main():
    #get_urls()
    #get_data()
    dataCollect()
    #get_data1()
    #dates()
    #get_results()


if __name__ == '__main__':
    main()
